import requests
from bs4 import BeautifulSoup

URL = "https://realpython.github.io/fake-jobs/"
page = requests.get(URL)

soup = BeautifulSoup(page.content, "html.parser")





#describes element id to print from
'''
results = soup.find(id="ResultsContainer")
print(results.prettify())
'''

#this finds all divs with the class card-content it includes all the job descriptions
'''
results = soup.find(id="ResultsContainer")
job_elements = results.find_all("div", class_="card-content")
for job_element in job_elements:
    print(job_element, end="\n"*2)
'''


# scraping specific elements from ResultsContainer
'''
for job_element in job_elements:
    title_element = job_element.find("h2", class_="title")
    company_element = job_element.find("h3", class_="company")
    location_element = job_element.find("p", class_="location")
    print(title_element)
    print(company_element)
    print(location_element)
    print() 
'''

# cleans up results of output 
'''
results = soup.find(id="ResultsContainer")
job_elements = results.find_all("div", class_="card-content")
for job_element in job_elements:
    title_element = job_element.find("h2", class_="title")
    company_element = job_element.find("h3", class_="company")
    location_element = job_element.find("p", class_="location")
    print(title_element.text.strip())
    print(company_element.text.strip())
    print(location_element.text.strip())
    print()
'''

# finds number of search results matching - no results use of string
'''
results = soup.find(id="ResultsContainer")
python_jobs = results.find_all("h2", string="Python")
print(python_jobs)
'''
# finds number of search results matching using the lambda text function
'''
results = soup.find(id="ResultsContainer")
python_jobs = results.find_all(
    "h2", string=lambda text: "python" in text.lower()
)
print(len(python_jobs))
'''

# access parent elements and print results 
'''
results = soup.find(id="ResultsContainer")
python_jobs = results.find_all(
    "h2", string=lambda text: "python" in text.lower()
)

python_job_elements = [
    h2_element.parent.parent.parent for h2_element in python_jobs
]
print(python_job_elements)
'''

# print learn and apply from bottom of div the .text attribute leaves only the visible content of an HTML element - strips everything but link text
'''
results = soup.find(id="ResultsContainer")
python_jobs = results.find_all(
    "h2", string=lambda text: "python" in text.lower()
)

python_job_elements = [
    h2_element.parent.parent.parent for h2_element in python_jobs
]

for job_element in python_job_elements:
    # -- snip --
    links = job_element.find_all("a")
    for link in links:
        print(link.text.strip())
'''

# provides links to at bottom of the page to apply for python jobs 
'''
results = soup.find(id="ResultsContainer")
python_jobs = results.find_all(
    "h2", string=lambda text: "python" in text.lower()
)

python_job_elements = [
    h2_element.parent.parent.parent for h2_element in python_jobs
]
for job_element in python_job_elements:
    # -- snip --
    links = job_element.find_all("a")
    for link in links:
        link_url = link["href"]
        print(f"Apply here: {link_url}\n")
'''
# fetches only the correct link to apply for python jobs
'''
results = soup.find(id="ResultsContainer")
python_jobs = results.find_all(
    "h2", string=lambda text: "python" in text.lower()
)

python_job_elements = [
    h2_element.parent.parent.parent for h2_element in python_jobs
]
for job_element in python_job_elements:
    # -- snip --
    links = job_element.find_all("a")
    for link in links:
        link_url = job_element.find_all("a")[1]["href"]
        print(f"Apply here: {link_url}\n")
'''        
